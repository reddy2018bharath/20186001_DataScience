{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "import seaborn.apionly as sns\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "def leppard(source_data, prediction_data):\n",
    "    false_negative = 0\n",
    "    false_positive = 0\n",
    "    correct_assessment = 0\n",
    "    for result in range(0, len(prediction_data)):\n",
    "        if int(prediction_data[result]) == 1 and int(source_data[result]) == 0:\n",
    "            false_positive += 1\n",
    "        if int(prediction_data[result]) == 0 and int(source_data[result]) == 1:\n",
    "            false_negative += 1\n",
    "        if (int(prediction_data[result]) == 1 and int(source_data[result]) == 1) or (int(prediction_data[result]) == 0 and int(source_data[result]) == 0):\n",
    "            correct_assessment += 1\n",
    "    print ()\n",
    "    print (\"False Positives: \", false_positive)\n",
    "    print (\"False Negatives: \", false_negative)\n",
    "    print (\"Correct Assessment: \", correct_assessment)\n",
    "\n",
    "    print (\"Classification Accuracy: \", 1 - (false_positive + false_negative) / len(source_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(9001)\n",
    "df = pd.read_csv('hw6_dataset.csv')\n",
    "msk = np.random.rand(len(df)) < 0.75\n",
    "data_train = df[msk]\n",
    "data_test = df[~msk]\n",
    "orig_columns = list(data_train.columns.values)\n",
    "new_columns = []\n",
    "for x in range (len(orig_columns) - 1):\n",
    "    #print(orig_columns[x])\n",
    "    index_of_e = orig_columns[x].index('e')\n",
    "    revised_string = orig_columns[x][:index_of_e + 4]\n",
    "    #print(revised_string)\n",
    "    converted_string = float(revised_string)\n",
    "    new_columns.append(str(converted_string))\n",
    "new_columns.append('Class Label')\n",
    "#print(new_columns)\n",
    "data_train.columns = new_columns\n",
    "data_test.columns = new_columns\n",
    "data_train.head(10)\n",
    "\n",
    "y_train = data_train['Class Label'].values\n",
    "X_train = data_train.values\n",
    "y_train = y_train.reshape(len(y_train), 1)\n",
    "\n",
    "y_test = data_test['Class Label'].values\n",
    "X_test = data_test.values\n",
    "y_test = y_test.reshape(len(y_test), 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-1.439999999999999891e-01</th>\n",
       "      <th>-1.429999999999999882e-01</th>\n",
       "      <th>-1.160000000000000059e-01</th>\n",
       "      <th>-1.029999999999999943e-01</th>\n",
       "      <th>2.260000000000000064e-01</th>\n",
       "      <th>2.099999999999999922e-01</th>\n",
       "      <th>-9.799999999999999822e-01</th>\n",
       "      <th>-7.800000000000000266e-01</th>\n",
       "      <th>-4.739999999999999769e-01</th>\n",
       "      <th>-4.470000000000000084e-01</th>\n",
       "      <th>...</th>\n",
       "      <th>9.250000000000000444e-01</th>\n",
       "      <th>5.160000000000000142e-01</th>\n",
       "      <th>3.439999999999999725e-01</th>\n",
       "      <th>9.060000000000000275e-01</th>\n",
       "      <th>-1.129999999999999893e+00</th>\n",
       "      <th>-5.520000000000000462e-01</th>\n",
       "      <th>5.530000000000000471e-01</th>\n",
       "      <th>-4.169999999999999818e-01</th>\n",
       "      <th>2.560000000000000053e-01</th>\n",
       "      <th>0.000000000000000000e+00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.01100</td>\n",
       "      <td>0.138</td>\n",
       "      <td>-0.2230</td>\n",
       "      <td>-0.1730</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.284</td>\n",
       "      <td>-0.0522</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.427</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.593</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.00785</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>-0.0789</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-0.0723</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21200</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>0.2660</td>\n",
       "      <td>0.2320</td>\n",
       "      <td>-1.190</td>\n",
       "      <td>-1.150</td>\n",
       "      <td>-1.8100</td>\n",
       "      <td>-1.560</td>\n",
       "      <td>-1.250</td>\n",
       "      <td>-1.200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.816</td>\n",
       "      <td>1.570</td>\n",
       "      <td>0.39400</td>\n",
       "      <td>1.340</td>\n",
       "      <td>-1.1800</td>\n",
       "      <td>-2.700</td>\n",
       "      <td>-0.926</td>\n",
       "      <td>-2.650</td>\n",
       "      <td>-0.0447</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.21500</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.0494</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.463</td>\n",
       "      <td>-1.0500</td>\n",
       "      <td>-0.941</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.37100</td>\n",
       "      <td>0.859</td>\n",
       "      <td>-0.9930</td>\n",
       "      <td>-0.492</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-0.0528</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.27900</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>0.0973</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-1.3200</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-1.110</td>\n",
       "      <td>-1.090</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.640</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.29500</td>\n",
       "      <td>0.403</td>\n",
       "      <td>-1.1200</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>0.468</td>\n",
       "      <td>-0.820</td>\n",
       "      <td>0.4350</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00922</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>0.1690</td>\n",
       "      <td>0.1540</td>\n",
       "      <td>-0.391</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-1.6900</td>\n",
       "      <td>-1.450</td>\n",
       "      <td>-0.546</td>\n",
       "      <td>-0.527</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.37100</td>\n",
       "      <td>0.481</td>\n",
       "      <td>-1.0600</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>0.550</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>0.1550</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   -1.439999999999999891e-01  -1.429999999999999882e-01  \\\n",
       "0                   -0.01100                      0.138   \n",
       "1                    0.21200                     -0.313   \n",
       "2                    0.21500                     -0.184   \n",
       "3                    0.27900                     -0.197   \n",
       "4                    0.00922                     -0.138   \n",
       "\n",
       "   -1.160000000000000059e-01  -1.029999999999999943e-01  \\\n",
       "0                    -0.2230                    -0.1730   \n",
       "1                     0.2660                     0.2320   \n",
       "2                     0.0274                     0.0494   \n",
       "3                     0.1270                     0.0973   \n",
       "4                     0.1690                     0.1540   \n",
       "\n",
       "   2.260000000000000064e-01  2.099999999999999922e-01  \\\n",
       "0                     0.188                     0.284   \n",
       "1                    -1.190                    -1.150   \n",
       "2                     0.443                     0.463   \n",
       "3                    -0.213                    -0.150   \n",
       "4                    -0.391                    -0.397   \n",
       "\n",
       "   -9.799999999999999822e-01  -7.800000000000000266e-01  \\\n",
       "0                    -0.0522                     -0.256   \n",
       "1                    -1.8100                     -1.560   \n",
       "2                    -1.0500                     -0.941   \n",
       "3                    -1.3200                     -0.994   \n",
       "4                    -1.6900                     -1.450   \n",
       "\n",
       "   -4.739999999999999769e-01  -4.470000000000000084e-01  ...  \\\n",
       "0                      0.129                      0.427  ...   \n",
       "1                     -1.250                     -1.200  ...   \n",
       "2                     -0.531                     -0.394  ...   \n",
       "3                     -1.110                     -1.090  ...   \n",
       "4                     -0.546                     -0.527  ...   \n",
       "\n",
       "   9.250000000000000444e-01  5.160000000000000142e-01  \\\n",
       "0                    -0.593                     0.452   \n",
       "1                    -0.816                     1.570   \n",
       "2                     0.634                     0.111   \n",
       "3                    -0.640                     0.485   \n",
       "4                    -0.277                     0.699   \n",
       "\n",
       "   3.439999999999999725e-01  9.060000000000000275e-01  \\\n",
       "0                   0.00785                    -0.533   \n",
       "1                   0.39400                     1.340   \n",
       "2                   0.37100                     0.859   \n",
       "3                   0.29500                     0.403   \n",
       "4                   0.37100                     0.481   \n",
       "\n",
       "   -1.129999999999999893e+00  -5.520000000000000462e-01  \\\n",
       "0                    -0.0789                      0.705   \n",
       "1                    -1.1800                     -2.700   \n",
       "2                    -0.9930                     -0.492   \n",
       "3                    -1.1200                     -0.343   \n",
       "4                    -1.0600                     -0.526   \n",
       "\n",
       "   5.530000000000000471e-01  -4.169999999999999818e-01  \\\n",
       "0                     0.906                      0.216   \n",
       "1                    -0.926                     -2.650   \n",
       "2                     0.363                      0.326   \n",
       "3                     0.468                     -0.820   \n",
       "4                     0.550                     -0.284   \n",
       "\n",
       "   2.560000000000000053e-01  0.000000000000000000e+00  \n",
       "0                   -0.0723                       0.0  \n",
       "1                   -0.0447                       0.0  \n",
       "2                   -0.0528                       0.0  \n",
       "3                    0.4350                       0.0  \n",
       "4                    0.1550                       0.0  \n",
       "\n",
       "[5 rows x 118 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\akshitha\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The optimized L2 regularization paramater id: [10.]\n",
      "Estimated beta1: \n",
      " [[-2.15675703e-02  4.57672332e-02  5.87118913e-01  5.01597809e-01\n",
      "  -2.54420819e-01 -2.18567419e-01  1.68100061e-01 -7.42716068e-02\n",
      "   1.15897470e-01 -3.84389984e-02 -5.24469406e-02  1.31078405e-01\n",
      "  -2.18614609e-01  1.63000945e-02  2.81643811e-02 -1.40624669e-02\n",
      "  -7.47535797e-02 -7.52547705e-02 -1.01655871e-01 -9.82645625e-02\n",
      "   1.57753700e-01  1.90654416e-02  8.81488442e-02 -3.22963562e-02\n",
      "   2.91600410e-01 -5.44620414e-02  2.61948057e-02 -9.40194824e-02\n",
      "  -1.88076526e-02 -3.40947720e-02 -4.57707437e-02  7.21185942e-02\n",
      "   2.34914779e-01  8.33270520e-02  4.96411398e-02 -6.72526432e-02\n",
      "   6.36391071e-02  5.50162518e-02 -1.41451473e-02  1.01859455e-02\n",
      "   1.31628157e-02  1.98233756e-02  2.33209299e-02 -7.50357087e-02\n",
      "   2.06580962e-02  3.89939132e-02  5.63545215e-02 -1.72732070e-01\n",
      "  -1.56078573e-01 -6.21999362e-02 -1.59293862e-01  1.10459109e-01\n",
      "  -4.91734159e-02 -3.82534716e-02 -3.06234710e-02 -2.53074489e-02\n",
      "  -1.69751568e-02  2.64366828e-01 -2.61942753e-01  7.40221152e-02\n",
      "  -5.09557862e-02  7.36473382e-02  7.21480995e-02  8.75476189e-02\n",
      "   9.49586260e-02  1.34815231e-02  2.39635118e-01 -1.65466522e-01\n",
      "  -1.22649176e-02  1.44632319e-02 -3.08337949e-02  3.58424274e-02\n",
      "   1.01931100e-01  1.34155224e-02  1.07915202e-02  8.49930542e-03\n",
      "   3.31735399e-02  8.11659796e-02  1.02843521e-02  7.39264328e-02\n",
      "   7.17942152e-02 -3.67858015e-02 -3.15508824e-02 -6.81644530e-03\n",
      "   3.21578138e-02  3.28756542e-02  6.09449688e-02  1.18699500e-01\n",
      "   1.63958777e-01 -4.67669594e-02 -5.25847199e-02 -5.38052815e-02\n",
      "  -1.15033864e-01 -4.30447497e-02  3.07032105e-02 -2.42748743e-01\n",
      "  -6.27315426e-02 -1.81842220e-02 -1.08988656e-02  1.19280910e-01\n",
      "   1.30252931e-01  6.46796744e-02 -1.83790329e-01 -4.91054664e-02\n",
      "   1.48516937e-02 -1.34156514e-02 -7.81414681e-02  1.96245515e-02\n",
      "   7.85621568e-02 -6.15018827e-02 -8.59113665e-02 -2.00210051e-01\n",
      "  -5.88035764e-02 -7.06945368e-02  8.43177834e-03  7.70496118e-02\n",
      "  -5.77997557e-02  1.12553641e+01]]\n",
      "Estimated beta0: \n",
      " [-9.51671301]\n",
      "\n",
      "\n",
      "malignant:  103.0\n",
      "\n",
      "\n",
      "Classifier applied to Test Set:\n",
      "\n",
      "False Positives:  0\n",
      "False Negatives:  1\n",
      "Correct Assessment:  17087\n",
      "Classification Accuracy:  0.9999414794007491\n",
      "[[16984     0]\n",
      " [    1   103]]\n",
      "\n",
      "\n",
      "Classifier that predicts all normal:\n",
      "\n",
      "False Positives:  0\n",
      "False Negatives:  104\n",
      "Correct Assessment:  16984\n",
      "Classification Accuracy:  0.9939138576779026\n",
      "[[16984     0]\n",
      " [  104     0]]\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegressionCV(\n",
    "        Cs=list(np.power(10.0, np.arange(-10, 10)))\n",
    "        ,penalty='l2'\n",
    "        ,cv=10\n",
    "        ,random_state=777\n",
    "        ,fit_intercept=True\n",
    "        ,solver='newton-cg'\n",
    "        ,tol=10)\n",
    "clf.fit(X_train, y_train)\n",
    "print('\\n')\n",
    "print(\"The optimized L2 regularization paramater id:\", clf.C_)\n",
    "\n",
    "# The coefficients\n",
    "print('Estimated beta1: \\n', clf.coef_)\n",
    "print('Estimated beta0: \\n', clf.intercept_)\n",
    "\n",
    "# Scoring\n",
    "clf_y_pred_test = clf.predict(X_test)\n",
    "clf_y_pred_test = clf_y_pred_test.reshape(len(clf_y_pred_test), 1)\n",
    "test_df = pd.DataFrame(clf_y_pred_test)\n",
    "Total = test_df[0].sum()\n",
    "print('\\n')\n",
    "print(\"malignant: \", Total)\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "test_df['All Normal'] = 0\n",
    "\n",
    "# Reset indexes so copy will work\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "data_test = data_test.reset_index(drop=True)\n",
    "test_df['Class Label'] = data_test['Class Label']\n",
    "\n",
    "# Confusion Matrix\n",
    "print('\\n')\n",
    "print('Classifier applied to Test Set:') \n",
    "leppard(test_df['Class Label'], test_df[0])\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print('Classifier that predicts all normal:')\n",
    "leppard(test_df['Class Label'], test_df['All Normal'])\n",
    "print(confusion_matrix(y_test, test_df['All Normal']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix that predicts all patients to be negative:\n",
      "[[16984     0]\n",
      " [  104     0]]\n"
     ]
    }
   ],
   "source": [
    "def t_repredict(est, t, xtest):\n",
    "    probs = est.predict_proba(xtest)\n",
    "    p0 = probs[:,0]\n",
    "    p1 = probs[:,1]\n",
    "    ypred = (p1 > t)*1\n",
    "    return ypred\n",
    "print('Confusion matrix that predicts all patients to be negative:')\n",
    "print(confusion_matrix(y_test,t_repredict(clf, 01.00, X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def make_roc(name, clf, ytest, xtest, ax=None, labe=5, proba=True, skip=0):\n",
    "    initial=False\n",
    "    if not ax:\n",
    "        ax=plt.gca()\n",
    "        initial=True\n",
    "    if proba:#for stuff like logistic regression\n",
    "        fpr, tpr, thresholds=roc_curve(ytest, clf.predict_proba(xtest)[:,1])\n",
    "    else:#for stuff like SVM\n",
    "        fpr, tpr, thresholds=roc_curve(ytest, clf.decision_function(xtest))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    if skip:\n",
    "        l=fpr.shape[0]\n",
    "        ax.plot(fpr[0:l:skip], tpr[0:l:skip], '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n",
    "    else:\n",
    "        ax.plot(fpr, tpr, '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n",
    "    label_kwargs = {}\n",
    "    label_kwargs['bbox'] = dict(\n",
    "        boxstyle='round,pad=0.3', alpha=0.2,\n",
    "    )\n",
    "    if labe!=None:    \n",
    "        for k in range(0, fpr.shape[0],labe):\n",
    "            #from https://gist.github.com/podshumok/c1d1c9394335d86255b8\n",
    "            threshold = str(np.round(thresholds[k], 2))\n",
    "            ax.annotate(threshold, (fpr[k], tpr[k]), **label_kwargs)\n",
    "    if initial:\n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title('ROC')\n",
    "    fpr_0, tpr_0, thresholds_1 = metrics.roc_curve(y_test, t_repredict(clf, 01.00, X_test))\n",
    "    roc_auc_0 = auc(fpr_0, tpr_0)\n",
    "    plt.plot(fpr_0, tpr_0, '.-', alpha=0.3, label='ROC curve for all Negative Predictions (area = %0.2f)' % (roc_auc_0))\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    return ax\n",
    "\n",
    "ax=make_roc(\"logistic\",clf, y_test, X_test, labe=100, skip=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17088"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clf.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR: 0.03144135657089025 TPR 1.0 Threshold 0.0005805368530553129\n",
      "FPR: 1.0 TPR 1.0 Threshold 8.233913585440455e-37\n",
      "FPR: 1.0 TPR 1.0 Threshold 8.233913585440455e-37\n",
      "FPR: 1.0 TPR 1.0 Threshold 8.233913585440455e-37\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fprs = [0,.1,.5,.9]\n",
    "fpr, tpr, thresholds=roc_curve(y_test, clf.predict_proba(X_test)[:,1])       \n",
    "for i in range(len(fpr)):        \n",
    "    if int(fpr[i] > 0):\n",
    "                print('FPR:', fpr[i], 'TPR', tpr[i], 'Threshold', thresholds[i] )\n",
    "                break\n",
    "for i in range(len(fpr)):             \n",
    "    if int(fpr[i]) >= .1:\n",
    "                print('FPR:', fpr[i], 'TPR', tpr[i], 'Threshold', thresholds[i] )\n",
    "                \n",
    "for i in range(len(fpr)):                 \n",
    "    if int(fpr[i]) >= .5:\n",
    "                print('FPR:', fpr[i], 'TPR', tpr[i], 'Threshold', thresholds[i] ) \n",
    "                \n",
    "for i in range(len(fpr)):      \n",
    "    if int(fpr[i]) >= .9:\n",
    "                print('FPR:', fpr[i], 'TPR', tpr[i], 'Threshold', thresholds[i] )  \n",
    "                \n",
    "                f, ax = plt.subplots()\n",
    "                ax.plot(fpr, tpr)\n",
    "                ax.set_xlabel('False Positive Rate')\n",
    "                ax.set_ylabel('True Positive Rate')\n",
    "                ax.set_title('ROC')\n",
    "                ax.legend(loc=\"lower right\")\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fprs: [0.         0.         0.         0.03144136 0.03155911 0.13565709\n",
      " 0.13577485 0.65667687 0.65679463 0.84979981 0.84991757 1.        ]\n"
     ]
    }
   ],
   "source": [
    "print('fprs:',fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tprs: [0.         0.00961538 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "print('tprs:',tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresholds: [1.99955010e+00 9.99550103e-01 4.40606742e-01 5.80536853e-04\n",
      " 5.80130001e-04 2.67749851e-04 2.67700932e-04 5.08770039e-05\n",
      " 5.08615873e-05 2.21872194e-05 2.21809271e-05 8.23391359e-37]\n"
     ]
    }
   ],
   "source": [
    "print('thresholds:',thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(9001)\n",
    "df = pd.read_csv('HW6_dataset_missing.csv')\n",
    "msk = np.random.rand(len(df)) < 0.75\n",
    "data_train = df[msk]\n",
    "data_test = df[~msk]\n",
    "data_train = data_train.dropna()\n",
    "data_test = data_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.1290</td>\n",
       "      <td>-0.2160</td>\n",
       "      <td>0.2880</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.9550</td>\n",
       "      <td>-1.620</td>\n",
       "      <td>-1.470</td>\n",
       "      <td>-1.0100</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.1900</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>0.395</td>\n",
       "      <td>2.060</td>\n",
       "      <td>-1.180</td>\n",
       "      <td>-2.8500</td>\n",
       "      <td>-1.290</td>\n",
       "      <td>-2.100</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0989</td>\n",
       "      <td>0.1160</td>\n",
       "      <td>0.3130</td>\n",
       "      <td>0.2810</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.2790</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.4320</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0181</td>\n",
       "      <td>0.2480</td>\n",
       "      <td>-0.869</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.6980</td>\n",
       "      <td>0.363</td>\n",
       "      <td>1.030</td>\n",
       "      <td>-0.2490</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.5790</td>\n",
       "      <td>0.5020</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>-0.2740</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.2160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0702</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.397</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.7380</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.440</td>\n",
       "      <td>-0.2880</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.2170</td>\n",
       "      <td>-0.3570</td>\n",
       "      <td>-0.0539</td>\n",
       "      <td>-0.0688</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.6380</td>\n",
       "      <td>0.436</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.0401</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0622</td>\n",
       "      <td>0.269</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>-1.030</td>\n",
       "      <td>0.0276</td>\n",
       "      <td>0.472</td>\n",
       "      <td>-0.390</td>\n",
       "      <td>0.3660</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.0846</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.4240</td>\n",
       "      <td>0.3520</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.0947</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.3020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7190</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>-0.704</td>\n",
       "      <td>0.8530</td>\n",
       "      <td>0.953</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.1190</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       1       2       3       4      5       6      7      8  \\\n",
       "0           0  0.1290 -0.2160  0.2880  0.2370 -0.993 -0.9550 -1.620 -1.470   \n",
       "1           1  0.0989  0.1160  0.3130  0.2810 -0.188 -0.2790  0.173  0.445   \n",
       "2           2  0.0215  0.1590  0.5790  0.5020 -0.342 -0.2740 -0.172 -0.164   \n",
       "3           3 -0.2170 -0.3570 -0.0539 -0.0688  0.445  0.6380  0.436  0.351   \n",
       "4           4 -0.0846  0.0166  0.4240  0.3520 -0.259 -0.0947  0.119 -0.162   \n",
       "\n",
       "        9  ...     109     110    111    112    113     114    115    116  \\\n",
       "0 -1.0100  ... -1.1900  1.1000  0.395  2.060 -1.180 -2.8500 -1.290 -2.100   \n",
       "1  0.4320  ... -0.0181  0.2480 -0.869 -0.190  0.451  0.6980  0.363  1.030   \n",
       "2  0.2160  ...  0.0702  0.0200  0.397 -0.800  0.173  0.7380  0.465  0.440   \n",
       "3  0.0401  ...     NaN  0.0622  0.269 -0.217 -1.030  0.0276  0.472 -0.390   \n",
       "4  0.3020  ...  0.7190  0.3250 -0.286 -0.528 -0.704  0.8530  0.953 -0.116   \n",
       "\n",
       "      117  type  \n",
       "0  0.0121   0.0  \n",
       "1 -0.2490   0.0  \n",
       "2 -0.2880   0.0  \n",
       "3  0.3660   0.0  \n",
       "4 -0.1190   0.0  \n",
       "\n",
       "[5 rows x 119 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_train['type'].values\n",
    "X_train = data_train.values\n",
    "y_train = y_train.reshape(len(y_train), 1)\n",
    "\n",
    "y_test = data_test['type'].values\n",
    "X_test = data_test.values\n",
    "y_test = y_test.reshape(len(y_test), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\akshitha\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\akshitha\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\model_selection\\_split.py:657: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "c:\\users\\akshitha\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\scipy\\optimize\\linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "c:\\users\\akshitha\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\scipy\\optimize\\linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The optimized L2 regularization paramater id: [1.e-10]\n",
      "Estimated beta1: \n",
      " [[-1.17808017e-05 -1.78234266e-11 -2.31175571e-11  1.41725318e-10\n",
      "   1.18659272e-10 -2.39497612e-10 -2.27439833e-10 -6.86226828e-11\n",
      "  -8.40750206e-11  8.00425581e-10  5.77491711e-10 -1.72371871e-11\n",
      "  -8.36812521e-12 -2.06867973e-10  6.54907848e-11 -6.54518372e-11\n",
      "  -6.05532801e-11 -2.35683854e-10 -5.53486215e-11  5.87403517e-11\n",
      "   1.83268337e-10  1.90492741e-10  6.96741116e-13  1.06296161e-10\n",
      "   1.56050918e-10  1.85817834e-10  1.57395059e-10  2.28944105e-10\n",
      "  -1.40218151e-10  2.34228351e-10 -1.28059283e-10  2.08089547e-11\n",
      "  -1.88814843e-11  2.92377986e-11  3.32047060e-11  2.46338585e-10\n",
      "   4.32449785e-11 -6.38750046e-11 -5.61772017e-11 -2.59920580e-11\n",
      "  -1.18540883e-10  1.28068940e-10  1.42189068e-10  1.53957248e-10\n",
      "   5.35933770e-11  1.81548039e-10  2.00309381e-10  1.35793683e-10\n",
      "  -2.70314930e-10 -1.51130963e-10 -1.37513811e-10 -1.39854945e-10\n",
      "  -2.97795542e-10 -8.24887496e-11 -3.18339890e-11 -3.57299019e-11\n",
      "  -3.63014006e-11 -3.69393480e-11  3.58260622e-10  3.50895706e-11\n",
      "   3.00861584e-10 -1.36340849e-10  3.06807682e-10  3.20711300e-10\n",
      "   3.30799922e-10  3.37652712e-10 -3.38124090e-10 -2.15076367e-10\n",
      "  -5.15707478e-10 -2.15609337e-10 -2.19685562e-10 -1.35648508e-10\n",
      "  -2.24650291e-10 -7.93260531e-11  6.54307575e-11  6.37001034e-11\n",
      "   6.15707943e-11  1.37642169e-10 -1.79896615e-10  6.92372540e-11\n",
      "  -1.68382371e-10 -1.66648767e-10  4.86087727e-11  9.02677590e-11\n",
      "   2.88771914e-11  2.92344017e-11 -1.25092885e-10 -1.22508366e-10\n",
      "  -1.19361202e-10 -1.87506846e-11 -2.20515462e-10 -2.19937178e-10\n",
      "  -2.19526295e-10 -5.52231870e-11 -6.57489114e-11 -1.53334664e-10\n",
      "  -6.93158524e-11 -1.47711913e-11 -5.54697183e-11  4.61865798e-11\n",
      "  -1.16783732e-12 -1.15688377e-10 -1.06032259e-10 -1.06277763e-10\n",
      "   3.31037504e-11 -3.99910592e-10 -2.25334914e-10 -6.10271987e-11\n",
      "  -2.33907084e-11  1.03020689e-10  9.80443814e-11 -1.61889181e-10\n",
      "   4.53439565e-11 -1.83528174e-10  8.66274070e-11  2.07356302e-10\n",
      "   1.07880489e-12  2.98871049e-11  1.97962077e-10]]\n",
      "Estimated beta0: \n",
      " [-4.43136374]\n",
      "\n",
      "\n",
      "Test Set Confusion matrix:\n",
      "[[325   0]\n",
      " [  1   0]]\n",
      "The training classification accuracy is:  0.9981981981981982\n",
      "The testing classification accuracy is:  0.9969325153374233\n",
      "The precision score on the test set is:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\akshitha\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegressionCV(\n",
    "        Cs=list(np.power(10.0, np.arange(-10, 10)))\n",
    "        ,penalty='l2'\n",
    "        ,cv=10\n",
    "        ,random_state=777\n",
    "        ,fit_intercept=True\n",
    "        ,solver='newton-cg'\n",
    "        ,tol=10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# L2 Regularization parameter\n",
    "print('\\n')\n",
    "print(\"The optimized L2 regularization paramater id:\", clf.C_)\n",
    "\n",
    "# The coefficients\n",
    "print('Estimated beta1: \\n', clf.coef_)\n",
    "print('Estimated beta0: \\n', clf.intercept_)\n",
    "\n",
    "# Metrics\n",
    "print('\\n')\n",
    "print('Test Set Confusion matrix:') \n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "\n",
    "train_score = clf.score(X_train, y_train)\n",
    "test_score = clf.score(X_test, y_test)\n",
    "y_prediction = clf.predict(X_test)\n",
    "test_precision = precision_score(y_test, y_prediction)\n",
    "print('The training classification accuracy is: ', train_score)\n",
    "print('The testing classification accuracy is: ', test_score)\n",
    "print('The precision score on the test set is: ', test_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(9001)\n",
    "df_2 = pd.read_csv('HW6_dataset_missing.csv')\n",
    "msk = np.random.rand(len(df)) < 0.75\n",
    "data_train_2 = df_2[msk]\n",
    "data_test_2 = df_2[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\akshitha\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "c:\\users\\akshitha\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\users\\akshitha\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The optimized L2 regularization paramater id: [10.]\n",
      "Estimated beta1: \n",
      " [[-4.09829729e-05 -1.11597651e-01  1.90828495e-01  3.12083456e-01\n",
      "   2.70588979e-01 -2.44093318e-01 -2.51309857e-01  1.00144970e-01\n",
      "   6.33574371e-02  1.02463399e-01 -7.45706563e-02  5.89673270e-02\n",
      "   3.70456779e-02 -1.50827027e-01 -7.65060250e-03 -1.54938053e-01\n",
      "  -2.41474560e-01  3.95173677e-01 -7.27504605e-02 -4.89959500e-02\n",
      "   1.49628333e-01  5.94759450e-02 -1.11637424e-01  1.98428539e-01\n",
      "   5.04326598e-01  2.60594337e-01  1.09637813e-01  4.29697744e-02\n",
      "  -1.25814415e-01 -7.70718519e-02  1.50361836e-01 -1.18808162e-01\n",
      "  -1.30270895e-01  2.03546856e-01  1.64047171e-01  5.28007894e-02\n",
      "   1.38039847e-01 -1.88845988e-01 -1.42547268e-01  1.38626011e-01\n",
      "   1.66398258e-01 -1.39268905e-01 -1.01931250e-01 -1.15335670e-01\n",
      "  -1.11961390e-01  1.56329743e-01  1.31916600e-01  1.39991727e-01\n",
      "  -2.55843451e-01 -7.29622670e-02  2.11638136e-02 -8.86888583e-02\n",
      "   5.28521562e-02  4.11360491e-04 -2.83490112e-02 -2.72391296e-02\n",
      "  -2.92612081e-02 -3.22963384e-02  3.21482646e-01 -1.29736014e-01\n",
      "   5.09829946e-02  5.08234834e-02  5.42102773e-02  8.43969667e-02\n",
      "   1.06445594e-01  1.20389022e-01  2.37319530e-01  9.69754578e-02\n",
      "   1.10494138e-01 -1.96078206e-02 -1.58164284e-02 -9.21822232e-03\n",
      "  -2.23382823e-02 -5.56859575e-02  1.05040669e-01  5.28460586e-02\n",
      "   4.55393681e-03 -1.91880819e-01  2.26358202e-01 -8.32613949e-02\n",
      "   1.09995168e-01  8.94647228e-02 -1.94371616e-01  2.92688625e-01\n",
      "   1.49277703e-02 -1.48302769e-02 -1.46232384e-01 -6.60795025e-02\n",
      "   4.10981722e-02  1.41361488e-01 -6.15904085e-03 -1.41687294e-02\n",
      "  -2.34030127e-02 -1.98919300e-01 -3.61404972e-02  8.30338958e-02\n",
      "   1.35873339e-01  2.77482239e-02 -2.07885378e-03 -6.98130950e-02\n",
      "   1.13614838e-01  2.78189118e-01 -7.19376952e-02 -1.83724726e-02\n",
      "  -1.87746022e-01 -2.56638508e-01 -1.03717163e-01 -8.66153832e-02\n",
      "   9.36417876e-02  9.00948245e-02 -7.81097864e-02 -2.32808575e-01\n",
      "  -2.29341858e-01 -3.42574989e-01  1.15997746e-01  2.92968104e-01\n",
      "  -7.80714919e-02  5.52748510e-02  9.83044881e+00]]\n",
      "Estimated beta0: \n",
      " [-8.26654633]\n",
      "\n",
      "\n",
      "Test Set Confusion matrix:\n",
      "[[6074    0]\n",
      " [   3   48]]\n",
      "The training classification accuracy is:  1.0\n",
      "The testing classification accuracy is:  0.9995102040816326\n",
      "The precision score on the test set is:  1.0\n"
     ]
    }
   ],
   "source": [
    "for column in data_train_2:\n",
    "    data_train_2[column] = data_train_2[column].fillna(data_train_2[column].mean())\n",
    "for column in data_test_2:\n",
    "    data_test_2[column] = data_test_2[column].fillna(data_train_2[column].mean())\n",
    "    \n",
    "y_train = data_train_2['type'].values\n",
    "X_train = data_train_2.values\n",
    "y_train = y_train.reshape(len(y_train), 1)\n",
    "\n",
    "y_test = data_test_2['type'].values\n",
    "X_test = data_test_2.values\n",
    "y_test = y_test.reshape(len(y_test), 1)\n",
    "\n",
    "\n",
    "# Fit a logistic regression classifier to the training set and report the accuracy of the classifier on the test set\n",
    "clf = LogisticRegressionCV(\n",
    "        Cs=list(np.power(10.0, np.arange(-10, 10)))\n",
    "        ,penalty='l2'\n",
    "        ,cv=10\n",
    "        ,random_state=777\n",
    "        ,fit_intercept=True\n",
    "        ,solver='newton-cg'\n",
    "        ,tol=10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# L2 Regularization parameter\n",
    "print('\\n')\n",
    "print(\"The optimized L2 regularization paramater id:\", clf.C_)\n",
    "\n",
    "# The coefficients\n",
    "print('Estimated beta1: \\n', clf.coef_)\n",
    "print('Estimated beta0: \\n', clf.intercept_)\n",
    "\n",
    "# Metrics\n",
    "print('\\n')\n",
    "print('Test Set Confusion matrix:') \n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "\n",
    "train_score = clf.score(X_train, y_train)\n",
    "test_score = clf.score(X_test, y_test)\n",
    "y_prediction = clf.predict(X_test)\n",
    "test_precision = precision_score(y_test, y_prediction)\n",
    "print('The training classification accuracy is: ', train_score)\n",
    "print('The testing classification accuracy is: ', test_score)\n",
    "print('The precision score on the test set is: ', test_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        0.027\n",
       "2       -0.527\n",
       "4       -1.550\n",
       "7        1.270\n",
       "8        0.571\n",
       "         ...  \n",
       "24980    0.100\n",
       "24988   -0.634\n",
       "24989   -0.194\n",
       "24993   -1.930\n",
       "24998    1.260\n",
       "Name: 91, Length: 6125, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split the data set into a training set and a testing set\n",
    "np.random.seed(9001)\n",
    "df_imp = pd.read_csv('HW6_dataset_missing.csv')\n",
    "msk = np.random.rand(len(df)) < 0.75\n",
    "data_train_imp = df_imp[msk]\n",
    "#print(data_train_imp)\n",
    "data_test_imp = df_imp[~msk]\n",
    "data_train_full = data_train_imp.dropna()\n",
    "\n",
    "data_test_imp.iloc[:, 91]\n",
    "#y_train_imp = data_train['type'].values\n",
    "#X_train_imp = data_train.values\n",
    "#y_train_imp = y_train.reshape(len(y_train), 1)\n",
    "\n",
    "#y_test_imp = data_test_imp['type'].values\n",
    "#X_test_imp = data_test_imp.values\n",
    "#y_test_imp = y_test_imp.reshape(len(y_test), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-d6743fd6cfd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#print(X_train_imp)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#print(X_train_imp.shape())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0my_train_imp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train_imp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_imp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;31m#print(y_train_imp)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\akshitha\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5178\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5179\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5180\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5182\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "for i in range(1,117,1):\n",
    "    y_train_imp = data_train_full.iloc[:, i]\n",
    "    #print(y_train_imp)\n",
    "    #print(y_train_imp.shape())\n",
    "    X_train_imp = data_train_full.loc[:, data_train_full.columns != i]\n",
    "    #print(X_train_imp)\n",
    "    #print(X_train_imp.shape())\n",
    "    y_train_imp = y_train_imp.reshape(len(y_train_imp), 1)\n",
    "    #print(y_train_imp)\n",
    "    \n",
    "    # regress column i on all other columns with randomness\n",
    "    regress = LinearRegression()\n",
    "    regress.fit(X_train_imp,y_train_imp)\n",
    "    y_hat = regress.predict(X_train_imp)\n",
    "    \n",
    "    X_missing = data_test_imp[data_test_imp.iloc[:, i].isnull()]\n",
    "   \n",
    "    print (X_missing)\n",
    "    if not X_missing:\n",
    "        print(\"X \", i, \"complete; nothing missing\")\n",
    "        continue\n",
    "    else:\n",
    "        print(X_missing)\n",
    "        print(TEST_missing)\n",
    "        \n",
    "\n",
    "    y_missing = regress.predict(X_missing)\n",
    "    y_missing_noise = y_missing+np.random.normal(loc=0,scale=np.sqrt(mean_squared_error(y_train_imp,y_hat)),size=y_missing.shape[0])\n",
    "\n",
    "        \n",
    "     \n",
    "    missing_index = data_train_imp.i[data_train_imp.i.isnull()].index\n",
    "    missing_series = pd.Series(data = y_missing_noise, index = missing_index)\n",
    "    \n",
    "    #back to the data set with missingness and impute the predictions\n",
    "    data_train_imp2 = data_train_imp.copy()\n",
    "    data_train_imp2[i] = data_train_imp2[i].fillna(missing_series)\n",
    "    \n",
    "    # regress on test set\n",
    "    regress.fit(X_train_imp,y_train_imp)\n",
    "    y_hat = regress.predict(X_train_imp)\n",
    "    \n",
    "    X_missing = data_train_imp[data_train_imp.i.isnull()]\n",
    "    X_missing = X_missing.reshape(len(X_missing), 1)\n",
    "    y_missing = regress.predict(X_missing)\n",
    "    y_missing_noise = y_missing+np.random.normal(loc=0,scale=np.sqrt(mean_squared_error(y_train_imp,y_hat)),size=y_missing.shape[0])\n",
    "    \n",
    "    missing_index = data_train_imp.i[data_train_imp.i.isnull()].index\n",
    "    missing_series = pd.Series(data = y_missing_noise, index = missing_index)\n",
    "    \n",
    "    #back to the data set with missingness and impute the predictions\n",
    "    data_train_imp2 = data_train_imp.copy()\n",
    "    data_train_imp2[i] = data_train_imp2[i].fillna(missing_series)\n",
    "    \n",
    "    # Fit a logistic regression classifier to the training set and report the accuracy of the classifier on the test set\n",
    "    clf = LogisticRegressionCV(\n",
    "        Cs=list(np.power(10.0, np.arange(-10, 10)))\n",
    "        ,penalty='l2'\n",
    "        ,cv=10\n",
    "        ,random_state=777\n",
    "        ,fit_intercept=True\n",
    "        ,solver='newton-cg'\n",
    "        ,tol=10)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # L2 Regularization parameter\n",
    "    print('\\n')\n",
    "    print(\"The optimized L2 regularization paramater id:\", clf.C_)\n",
    "\n",
    "    # The coefficients\n",
    "    print('Estimated beta1: \\n', clf.coef_)\n",
    "    print('Estimated beta0: \\n', clf.intercept_)\n",
    "\n",
    "    # Metrics\n",
    "    print('\\n')\n",
    "    print('Test Set Confusion matrix:') \n",
    "    print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "    \n",
    "    train_score = clf.score(X_train, y_train)\n",
    "    test_score = clf.score(X_test, y_test)\n",
    "    y_prediction = clf.predict(X_test)\n",
    "    test_precision = precision_score(y_test, y_prediction)\n",
    "    print('The training classification accuracy is: ', train_score)\n",
    "    print('The testing classification accuracy is: ', test_score)\n",
    "    print('The precision score on the test set is: ', test_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
